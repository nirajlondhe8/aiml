{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtE5/kuEpGt7Qp2YtoSiBt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emGZph883G9K"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install Necessary Libraries\n",
        "# Install the required libraries\n",
        "!pip install causalnex scikit-learn\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from causalnex.structure.notears import from_pandas\n",
        "from causalnex.inference import InferenceEngine\n",
        "import networkx as nx\n",
        "\n",
        "# Set seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Step 3: Generate Sample Data\n",
        "# Define the number of samples\n",
        "n_samples = 1000\n",
        "\n",
        "# Define network functions\n",
        "network_functions = ['GNB', 'AMF', 'SMF', 'UPF', 'AUSF', 'UDM', 'UDR', 'HSS', 'PGW', 'SGW']\n",
        "\n",
        "# Initialize an empty dataframe\n",
        "data = pd.DataFrame()\n",
        "\n",
        "# Generate sample data for each network function\n",
        "for nf in network_functions:\n",
        "    data[f'{nf}_cpu_usage'] = np.random.normal(loc=50, scale=10, size=n_samples)\n",
        "    data[f'{nf}_memory_usage'] = np.random.normal(loc=30, scale=5, size=n_samples)\n",
        "    data[f'{nf}_packet_loss'] = np.random.normal(loc=0.5, scale=0.1, size=n_samples)\n",
        "    data[f'{nf}_latency'] = np.random.normal(loc=10, scale=2, size=n_samples)\n",
        "\n",
        "# Introduce anomalies\n",
        "n_anomalies = 50\n",
        "anomaly_indices = np.random.choice(data.index, n_anomalies, replace=False)\n",
        "\n",
        "# Randomly introduce anomalies in CPU usage for demonstration\n",
        "for nf in network_functions:\n",
        "    data.loc[anomaly_indices, f'{nf}_cpu_usage'] = np.random.normal(loc=80, scale=5, size=n_anomalies)\n",
        "\n",
        "print(\"Sample Data with Anomalies:\")\n",
        "print(data.head())\n",
        "\n",
        "# Step 4: Unsupervised Anomaly Detection using Isolation Forest\n",
        "# Define the Isolation Forest model\n",
        "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "iso_forest.fit(data)\n",
        "\n",
        "# Predict anomalies\n",
        "data['anomaly'] = iso_forest.predict(data)\n",
        "\n",
        "# Mark anomalies with -1 and normal points with 1\n",
        "data['anomaly'] = data['anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
        "\n",
        "print(\"\\nAnomaly Detection Results (Unsupervised - Isolation Forest):\")\n",
        "print(data['anomaly'].value_counts())\n",
        "\n",
        "# Step 5: Supervised Anomaly Detection using Random Forest\n",
        "# Assuming we had labels, we split the data\n",
        "X = data.drop(columns=['anomaly'])\n",
        "y = data['anomaly']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nSupervised Anomaly Detection Results (Random Forest):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 6: Root Cause Analysis using Causal DAGs\n",
        "# Constructing a Causal DAG from the data\n",
        "# Learn the DAG from the data using NOTEARS algorithm\n",
        "sm = from_pandas(data)\n",
        "\n",
        "# Visualize the DAG\n",
        "plt.figure(figsize=(12, 8))\n",
        "nx.draw(sm, with_labels=True, node_size=2000, node_color='skyblue', font_size=12, font_weight='bold')\n",
        "plt.title('Causal DAG Structure')\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Performing Root Cause Analysis\n",
        "# Initialize the inference engine\n",
        "ie = InferenceEngine(sm)\n",
        "\n",
        "# Set evidence - assume an anomaly is detected in GNB CPU usage\n",
        "ie.observe({f'GNB_cpu_usage': 80})\n",
        "\n",
        "# Query the probabilities of the other nodes\n",
        "predictions = ie.query()\n",
        "print(\"\\nRoot Cause Analysis Predictions:\")\n",
        "print(predictions)\n"
      ]
    }
  ]
}